I"ﬁ)<p>For my final <a href="https://bit.ly/uwfsdl">CSEP 590C</a> project at UW (I‚Äôm finally graduating with my MS degreeüéâ), I wanted to apply deep learning techniques to an open source project that I maintain: <a href="https://github.com/lyft/cartography">Cartography</a>. Cartography is a Python tool that pulls data on technical assets from multiple sources, like <a href="https://aws.amazon.com/">AWS</a> and <a href="https://cloud.google.com/">GCP</a>, and represents them in a <a href="https://www.neo4j.com">Neo4j</a> graph database. Using a graph gives us a map of our infrastructure and this is particularly useful for security and infra teams because you can quickly answer complex questions like ‚Äúif one of my compute instances gets hacked, what else is immediately at risk?‚Äù</p>

<p><img src="https://github.com/lyft/cartography/raw/master/docs/images/accountsandrds.png" alt="a sample graph" /></p>

<p>This blog post goes over how I found a graph problem to apply deep learning techniques to, how I gave it the good ol‚Äô college try, and how I ended up with more questions than answers. I‚Äôll preface this by first saying that ‚ö†Ô∏èI am <strong>not</strong> a data scientist‚ö†Ô∏è so this post with embarassingly bad results is just me trying to apply ML techniques to a domain that I‚Äôm familiar with.</p>

<h1 id="finding-a-problem-for-my-solution">Finding a problem for my solution</h1>
<p>(lol)</p>

<p>While brainstorming, my main source of inspiration was <a href="https://neo4j.com/blog/deepwalk-implementing-graph-embeddings-in-neo4j/">Deep Walk: Implementing Graph Embeddings in Neo4j</a>, as well as <a href="https://arxiv.org/pdf/1403.6652.pdf">DeepWalk: Online Learning of Social Representations</a>.</p>

<p>The way I understood it, the DeepWalk technique represents each node in our graph as a vector embedding by taking random walks from each node for a certain number of hops. This is powerful because representing each graph node as a set of numbers potentially gives us a way to quantify how similar nodes are to each other based on their connections.</p>

<p>Embeddings are used in words and sentences in NLP as well, and it was shown in a <a href="https://arxiv.org/pdf/1301.3781.pdf">famous paper</a> that you can calculate the analogy <code class="language-plaintext highlighter-rouge">man : woman :: king : queen</code> by performing <code class="language-plaintext highlighter-rouge">king - man + woman</code>. I hoped to use vectors in this way with Cartography nodes.</p>

<p>It‚Äôs also super cool that <a href="https://github.com/syedfahadsultan">@fahadsultan</a> wrote <a href="https://github.com/syedfahadsultan/DeepWalkWithNeo">code to make these embeddings in Neo4j</a>, so I wouldn‚Äôt even need to work <em>that</em> hard.</p>

<h1 id="my-problem-auto-tag-aws-s3-buckets">My problem: auto-tag AWS S3 buckets</h1>
<p>Lyft‚Äôs architecture is made up of many many microservices. We use <a href="https://docs.aws.amazon.com/resourcegroupstagging/latest/APIReference/API_Tag.html">AWS tags</a> to organize assets based on which service they belong to. Not too long ago, I added support in Cartography for <a href="https://twitter.com/alexchantavy/status/1248022014331252736">AWS Tags</a>. Here‚Äôs what tags look like applied to S3 buckets. This lets us quickly find which S3 bucket belongs to which service.</p>

<p><img src="/assets/img/intro-tags.png" alt="intro image" /></p>

<p>However (and I‚Äôm sure almost anyone working in IT can identify), not everything is cataloged or tagged properly and there are many S3 buckets left without tags. It would be really great to have a computer automatically suggest tags for S3 buckets to a reasonable degree of accuracy. My intuition told me this should be possible because S3 buckets can be connected to more node types than just AWS tags and we should be able to infer the tag relationship based on other existing connections. As seen in the Cartography <a href="https://github.com/lyft/cartography/blob/master/docs/schema/aws.md#relationships-36">schema</a>,</p>

<ul>
  <li>
    <p>S3Buckets are resources in an AWS Account:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  (AWSAccount)-[RESOURCE]-&gt;(S3Bucket)
</code></pre></div>    </div>
  </li>
  <li>
    <p>S3 Access Control Lists apply to S3 buckets:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  (S3Acl)-[APPLIES_TO]-&gt;(S3Bucket)
</code></pre></div>    </div>
  </li>
  <li>
    <p>AWS users can read data from S3 buckets:</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  (AWSPrincipal)-[CAN_READ]-&gt;(S3Bucket)
</code></pre></div>    </div>
  </li>
</ul>

<p>Explained another way, it would be neat to hide the <code class="language-plaintext highlighter-rouge">(S3Bucket)-[TAGGED]-&gt;(AWSTag)</code> connection from our ML algorithm and see if it can guess the best tag for an S3 bucket based on the other relationships.</p>

<h1 id="my-experiment">My experiment</h1>

<h2 id="setup">Setup</h2>

<p>I started with a real Neo4j database populated with about 217,000 Cartography nodes and 430,000 edges. Of the nodes, 400 were S3 buckets with AWS service tags connected to them. It looked like this: <code class="language-plaintext highlighter-rouge">(S3Bucket{name:"abc"})-[:TAGGED]-&gt;(AWSTag{id:"service_name:my_service_name_here"}</code></p>

<p>Out of these 400 buckets, I deleted their connections to <code class="language-plaintext highlighter-rouge">service</code> tags from 80 of them. My idea was to predict tags for these remaining 80 buckets by assigning each of them the tag whose vector was ‚Äúclosest‚Äù to them. Explained in slightly more precise terms, my goal was to correctly perform</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">buckets</span><span class="p">:</span>
    <span class="n">bucket</span><span class="p">.</span><span class="n">tag</span> <span class="o">=</span> <span class="n">find_closest_tag</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>
</code></pre></div></div>

<p>where</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">find_closest_tag</span><span class="p">(</span><span class="n">bucket</span><span class="p">):</span>
    <span class="n">min_distance</span> <span class="o">=</span> <span class="n">INF</span>
    <span class="n">closest_tag</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">for</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">tags</span><span class="p">:</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">distance</span><span class="p">(</span><span class="n">embedding</span><span class="p">(</span><span class="n">bucket</span><span class="p">),</span> <span class="n">embedding</span><span class="p">(</span><span class="n">tag</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">dist</span> <span class="o">&lt;</span> <span class="n">min_distance</span><span class="p">:</span>
            <span class="n">min_distance</span> <span class="o">=</span> <span class="n">dist</span>
            <span class="n">closest_tag</span> <span class="o">=</span> <span class="n">tag</span>
    <span class="k">return</span> <span class="n">closest_tag</span>
</code></pre></div></div>

<p>The glaring assumption here is that the remaining 320 buckets with tags still attached to them fulfill this criteria as well. However at the time of this writing this is the best criteria that I could come up with.</p>

<h2 id="training-the-embeddings">Training the embeddings</h2>

<p>First I needed to make embeddings for each node in the graph. Again, lots of thanks to <a href="https://github.com/syedfahadsultan/DeepWalkWithNeo">DeepWalkWithNeo</a>. This took almost 12 hours to run on my 2018 Macbook Pro, but this is probably mostly because I ran the Python code as-is without bothering to think about parallelization; oh well.</p>

<p><img src="/assets/img/training.png" alt="training" /></p>

<p>In the end, each node got updated with an <code class="language-plaintext highlighter-rouge">embedding</code> field in the graph. You can see this for S3 buckets:</p>

<p><img src="/assets/img/bucket-embed.png" alt="bucket embeddings" /></p>

<p>as well as tags:</p>

<p><img src="/assets/img/tag-embed.png" alt="tag embeddings" /></p>

<h2 id="evaluation---so-did-it-work">Evaluation - so did it work?</h2>

<p>I was so excited by this point having finally found a pretty neat usecase for using a learning technique for graphs and I was eager to see the results.</p>

<p>I quickly threw together a script to find the closest tag for each bucket and got an accuracy of‚Ä¶
<img src="/assets/img/eval-results-redacted.png" alt="eval results" /></p>

<p>Yup, that‚Äôs 7.5% :-(.</p>

<p>Geesh.</p>

<p>What went wrong?!</p>

<p>It looks like many of these buckets are closet to one particular tag but I‚Äôm not sure why. I could redesign this calculation so that each bucket gets assigned a <em>unique</em> tag, but I think that‚Äôs too strong of an assumption because a tag can be assigned to multiple buckets.</p>

<h1 id="next-steps">Next steps</h1>
<p>If nothing else, this project has given me a <strong>lot</strong> to think about and work on. I want to try the following ideas:</p>

<ul>
  <li>Try the reverse direction: for each <strong>tag</strong>, find the closest bucket.</li>
  <li>Try different embedding techniques, such as those described in <a href="https://medium.com/octavian-ai/deep-learning-with-knowledge-graphs-3df0b469a61a">this post</a></li>
  <li>Find a way to reduce the time it takes to build embeddings in this graph (12 hours is awful)</li>
  <li>Try this same experiment but for labels other than S3 buckets. For example maybe I can try to tag EC2 instances instead, or maybe not even deal with tagging at all.</li>
</ul>

<p>Anyway, I hope you enjoyed this read. If you have any ideas on how I can get better results, please tweet me <a href="https://twitter.com/alexchantavy">@alexchantavy</a>. Do also let me know if you are working on or have solved any other graph-related ML problems! If you‚Äôre interested in <a href="https://github.com/lyft/cartography">Cartography</a>, please come say hi in our public Slack as we‚Äôd love to hear from you.</p>
:ET